{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "from anndata.experimental.pytorch import AnnLoader\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "genes_df = ad.read_h5ad(\"/work3/s193518/scIsoPred/data/bulk_processed_genes.h5ad\")\n",
    "t1 = time.time()\n",
    "\n",
    "print('loaded gene df in:', f'{t1-t0:.2f}', 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "isoform_df = ad.read_h5ad(\"/work3/s193518/scIsoPred/data/bulk_processed_transcripts.h5ad\")\n",
    "t1 = time.time()\n",
    "\n",
    "print('loaded isoform df in:', f'{t1-t0:.2f}', 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting dimensions\n",
    "print('number of genes =', genes_df.n_vars)\n",
    "print('number of isoforms =', isoform_df.n_vars)\n",
    "print('number of samples =', genes_df.n_obs)\n",
    "print('proportion =', f'1 gene : {isoform_df.n_vars/genes_df.n_vars:.1f} isoforms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Low-Expression Features\n",
    "Remove genes and isoforms with very low total counts across all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dense arrays for filtering\n",
    "genes_X = torch.from_numpy(genes_df.X.toarray()).float()\n",
    "isoforms_Y = torch.from_numpy(isoform_df.X.toarray()).float()\n",
    "\n",
    "print(f\"Original shapes: genes {genes_X.shape}, isoforms {isoforms_Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter genes with sum of counts < 20 across all samples\n",
    "MIN_COUNTS = 20\n",
    "\n",
    "gene_sums = genes_X.sum(dim=0)\n",
    "genes_to_keep = gene_sums >= MIN_COUNTS\n",
    "genes_X_filtered = genes_X[:, genes_to_keep]\n",
    "\n",
    "print(f\"Genes: {genes_X.shape[1]} -> {genes_X_filtered.shape[1]} (removed {(~genes_to_keep).sum()} low-expression genes)\")\n",
    "\n",
    "# Filter isoforms with sum of counts < 20 across all samples\n",
    "isoform_sums = isoforms_Y.sum(dim=0)\n",
    "isoforms_to_keep = isoform_sums >= MIN_COUNTS\n",
    "isoforms_Y_filtered = isoforms_Y[:, isoforms_to_keep]\n",
    "\n",
    "print(f\"Isoforms: {isoforms_Y.shape[1]} -> {isoforms_Y_filtered.shape[1]} (removed {(~isoforms_to_keep).sum()} low-expression isoforms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to use filtered data\n",
    "genes_X = genes_X_filtered\n",
    "isoforms_Y = isoforms_Y_filtered\n",
    "\n",
    "# Store filtered gene and isoform metadata\n",
    "genes_filtered_var = genes_df.var[genes_to_keep.numpy()]\n",
    "isoforms_filtered_var = isoform_df.var[isoforms_to_keep.numpy()]\n",
    "\n",
    "print(f\"\\nFinal dimensions: genes={genes_X.shape[1]}, isoforms={isoforms_Y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Isoform Proportions per Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_isoform_proportions(isoforms_Y, isoform_var_df):\n",
    "    \"\"\"\n",
    "    Convert isoform counts to proportions per gene.\n",
    "    For each gene, isoform proportions sum to 1.\n",
    "    \n",
    "    Args:\n",
    "        isoforms_Y: tensor of shape [n_samples, n_isoforms]\n",
    "        isoform_var_df: DataFrame with 'gene_id' column mapping isoforms to genes\n",
    "    \n",
    "    Returns:\n",
    "        proportions: tensor of shape [n_samples, n_isoforms] with proportions\n",
    "    \"\"\"\n",
    "    proportions = isoforms_Y.clone()\n",
    "    \n",
    "    # Group isoforms by gene\n",
    "    gene_ids = isoform_var_df['gene_id'].values\n",
    "    unique_genes = np.unique(gene_ids)\n",
    "    \n",
    "    print(f\"Converting counts to proportions for {len(unique_genes)} unique genes...\")\n",
    "    \n",
    "    # For each gene, normalize its isoforms to sum to 1\n",
    "    for gene_id in tqdm(unique_genes, desc=\"Processing genes\"):\n",
    "        # Find all isoforms belonging to this gene\n",
    "        isoform_mask = gene_ids == gene_id\n",
    "        isoform_indices = np.where(isoform_mask)[0]\n",
    "        \n",
    "        # Get counts for these isoforms across all samples\n",
    "        gene_isoform_counts = isoforms_Y[:, isoform_indices]  # [n_samples, n_isoforms_for_gene]\n",
    "        \n",
    "        # Sum across isoforms for this gene (per sample)\n",
    "        gene_total = gene_isoform_counts.sum(dim=1, keepdim=True) + 1e-8  # [n_samples, 1]\n",
    "        \n",
    "        # Convert to proportions\n",
    "        proportions[:, isoform_indices] = gene_isoform_counts / gene_total\n",
    "    \n",
    "    return proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert isoform counts to proportions\n",
    "isoforms_Y_proportions = convert_to_isoform_proportions(isoforms_Y, isoforms_filtered_var)\n",
    "\n",
    "# Verify some proportions sum to ~1 for each gene\n",
    "sample_gene = isoforms_filtered_var['gene_id'].values[0]\n",
    "sample_isoforms = isoforms_filtered_var['gene_id'].values == sample_gene\n",
    "print(f\"\\nSample verification for gene {sample_gene}:\")\n",
    "print(f\"Sum of proportions (sample 0): {isoforms_Y_proportions[0, sample_isoforms].sum():.4f}\")\n",
    "print(f\"Sum of proportions (sample 1): {isoforms_Y_proportions[1, sample_isoforms].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gene_to_isoform_mapping(isoform_var_df):\n",
    "    \"\"\"\n",
    "    create mapping from gene ids to their isoform indices.\n",
    "    used by the GeneAwareMLP class to apply softmax per gene.\n",
    "    \n",
    "    Returns:\n",
    "        gene_to_isoform_map: dict mapping gene_id -> list of isoform indices\n",
    "        sorted_genes: list of gene IDs in sorted order\n",
    "        isoform_to_position: dict mapping global isoform index -> (gene_idx, position_in_gene)\n",
    "    \"\"\"\n",
    "    gene_ids = isoform_var_df['gene_id'].values\n",
    "    unique_genes = np.unique(gene_ids)\n",
    "    \n",
    "    gene_to_isoform_map = {}\n",
    "    isoform_to_position = {}\n",
    "    \n",
    "    for gene_id in unique_genes:\n",
    "        # Find all isoforms for this gene\n",
    "        isoform_indices = np.where(gene_ids == gene_id)[0].tolist()\n",
    "        gene_to_isoform_map[gene_id] = isoform_indices\n",
    "        \n",
    "        # Store reverse mapping for reconstruction\n",
    "        for pos, iso_idx in enumerate(isoform_indices):\n",
    "            isoform_to_position[iso_idx] = (gene_id, pos)\n",
    "    \n",
    "    print(f\"Created mapping for {len(unique_genes)} genes\")\n",
    "    print(f\"Average isoforms per gene: {len(gene_ids) / len(unique_genes):.2f}\")\n",
    "    \n",
    "    # Sort genes for consistent ordering\n",
    "    sorted_genes = sorted(gene_to_isoform_map.keys())\n",
    "    \n",
    "    return gene_to_isoform_map, sorted_genes, isoform_to_position\n",
    "\n",
    "\n",
    "gene_to_isoform_map, sorted_gene_ids, isoform_to_position = create_gene_to_isoform_mapping(isoforms_filtered_var)\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample genes and their isoforms:\")\n",
    "for i, gene_id in enumerate(sorted_gene_ids[:3]):\n",
    "    n_isoforms = len(gene_to_isoform_map[gene_id])\n",
    "    print(f\"  {gene_id}: {n_isoforms} isoforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gene-to-Isoform Mapping for Gene-Aware Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed GenesDataset with Proper Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenesDataset(Dataset):\n",
    "    def __init__(self, genes_X, isoforms_Y, log_transform=True, normalize=True, axis='feature'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            genes_X (torch.Tensor): gene expression matrix [n_samples, n_genes]\n",
    "            isoforms_Y (torch.Tensor): isoform abundance matrix [n_samples, n_isoforms]\n",
    "            log_transform (bool): whether to apply log transform with pseudocount. Default = True\n",
    "            normalize (bool): whether to apply z-score normalization. Default = True\n",
    "            axis (str): 'feature' or 'sample' - axis along which to normalize\n",
    "        \"\"\"\n",
    "        # Clone to avoid modifying original data\n",
    "        self.genes_X = genes_X.clone()\n",
    "        self.isoforms_Y = isoforms_Y.clone()\n",
    "        \n",
    "        if log_transform:\n",
    "            # Log transform with pseudocount\n",
    "            self.genes_X = torch.log1p(self.genes_X)\n",
    "            self.isoforms_Y = torch.log1p(self.isoforms_Y)\n",
    "        \n",
    "        if normalize:\n",
    "            if axis == \"feature\":  # normalize per gene/isoform across samples\n",
    "                genes_mean = self.genes_X.mean(dim=0, keepdim=True)\n",
    "                genes_std = self.genes_X.std(dim=0, keepdim=True) + 1e-8\n",
    "                self.genes_X = (self.genes_X - genes_mean) / genes_std\n",
    "\n",
    "                iso_mean = self.isoforms_Y.mean(dim=0, keepdim=True)\n",
    "                iso_std = self.isoforms_Y.std(dim=0, keepdim=True) + 1e-8\n",
    "                self.isoforms_Y = (self.isoforms_Y - iso_mean) / iso_std\n",
    "\n",
    "            elif axis == \"sample\":  # normalize per cell/sample\n",
    "                genes_mean = self.genes_X.mean(dim=1, keepdim=True)\n",
    "                genes_std = self.genes_X.std(dim=1, keepdim=True) + 1e-8\n",
    "                self.genes_X = (self.genes_X - genes_mean) / genes_std\n",
    "\n",
    "                iso_mean = self.isoforms_Y.mean(dim=1, keepdim=True)\n",
    "                iso_std = self.isoforms_Y.std(dim=1, keepdim=True) + 1e-8\n",
    "                self.isoforms_Y = (self.isoforms_Y - iso_mean) / iso_std\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"axis must be either 'feature' or 'sample'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.genes_X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.genes_X[idx].unsqueeze(0), self.isoforms_Y[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full dataset instead of subset\n",
    "seed = 42\n",
    "test_size = 0.1\n",
    "\n",
    "np.random.seed(seed)\n",
    "n_obs = genes_df.n_obs\n",
    "indices = np.random.permutation(n_obs)\n",
    "\n",
    "test_count = int(n_obs * test_size)\n",
    "test_idx = indices[:test_count]\n",
    "train_idx = indices[test_count:]\n",
    "\n",
    "print(f\"Train samples: {len(train_idx)}\")\n",
    "print(f\"Test samples: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with isoform proportions\n",
    "train_dataset = GenesDataset(\n",
    "    genes_X[train_idx],\n",
    "    isoforms_Y_proportions[train_idx],\n",
    "    log_transform=True,\n",
    "    normalize=True,\n",
    "    axis='feature'\n",
    ")\n",
    "\n",
    "test_dataset = GenesDataset(\n",
    "    genes_X[test_idx],\n",
    "    isoforms_Y_proportions[test_idx],\n",
    "    log_transform=True,\n",
    "    normalize=True,\n",
    "    axis='feature'\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(train_dataset)} training and {len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene-Aware Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Gene-aware MLP that ensures isoform proportions sum to 1 per gene.\n",
    "    Uses a shared encoder followed by gene-specific output heads with softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, gene_to_isoform_map, sorted_gene_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: number of input genes\n",
    "            hidden_dims: list of hidden layer dimensions\n",
    "            gene_to_isoform_map: dict mapping gene_id -> list of isoform indices\n",
    "            sorted_gene_ids: sorted list of gene IDs for consistent ordering\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared encoder\n",
    "        encoder_layers = []\n",
    "        for in_size, out_size in zip([input_dim] + hidden_dims, hidden_dims):\n",
    "            encoder_layers.append(nn.Linear(in_size, out_size))\n",
    "            encoder_layers.append(nn.LayerNorm(out_size))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            encoder_layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Gene-specific output heads\n",
    "        # Each gene gets its own linear layer that outputs logits for its isoforms\n",
    "        self.gene_heads = nn.ModuleDict()\n",
    "        for gene_id in sorted_gene_ids:\n",
    "            n_isoforms = len(gene_to_isoform_map[gene_id])\n",
    "            # Linear layer: hidden_dim -> n_isoforms for this gene\n",
    "            self.gene_heads[str(gene_id)] = nn.Linear(hidden_dims[-1], n_isoforms)\n",
    "        \n",
    "        self.gene_to_isoform_map = gene_to_isoform_map\n",
    "        self.sorted_gene_ids = sorted_gene_ids\n",
    "        \n",
    "        # Pre-compute output order for efficient reconstruction\n",
    "        self._precompute_output_order()\n",
    "    \n",
    "    def _precompute_output_order(self):\n",
    "        \"\"\"\n",
    "        Precompute the mapping from gene outputs to final isoform tensor.\n",
    "        This avoids recomputing it every forward pass.\n",
    "        \"\"\"\n",
    "        self.output_mapping = []  # List of (gene_idx, isoform_positions)\n",
    "        \n",
    "        for gene_idx, gene_id in enumerate(self.sorted_gene_ids):\n",
    "            isoform_positions = self.gene_to_isoform_map[gene_id]\n",
    "            self.output_mapping.append((gene_idx, isoform_positions))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with gene-aware softmax.\n",
    "        \n",
    "        Returns:\n",
    "            isoform_proportions: tensor [batch, n_isoforms] where proportions\n",
    "                                 sum to 1 for each gene\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Shared encoding\n",
    "        encoded = self.encoder(x)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        total_isoforms = sum(len(positions) for positions in self.gene_to_isoform_map.values())\n",
    "        output = torch.zeros(batch_size, total_isoforms, device=x.device)\n",
    "        \n",
    "        # For each gene, predict isoform proportions with softmax\n",
    "        for gene_id in self.sorted_gene_ids:\n",
    "            # Get logits for this gene's isoforms\n",
    "            logits = self.gene_heads[str(gene_id)](encoded)  # [batch, n_isoforms_for_gene]\n",
    "            \n",
    "            # Apply softmax to ensure sum to 1\n",
    "            proportions = torch.softmax(logits, dim=1)  # [batch, n_isoforms_for_gene]\n",
    "            \n",
    "            # Place in correct positions in output tensor\n",
    "            isoform_indices = self.gene_to_isoform_map[gene_id]\n",
    "            output[:, isoform_indices] = proportions\n",
    "        \n",
    "        return output.unsqueeze(1)  # [batch, 1, n_isoforms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize GeneAwareMLP model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "hidden_dims = [2048, 1024, 512, 256, 128]\n",
    "\n",
    "model = GeneAwareMLP(\n",
    "    input_dim=genes_X.shape[1],  # filtered gene count\n",
    "    hidden_dims=hidden_dims,\n",
    "    gene_to_isoform_map=gene_to_isoform_map,\n",
    "    sorted_gene_ids=sorted_gene_ids\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Number of gene-specific heads: {len(model.gene_heads)}\")\n",
    "\n",
    "if use_cuda:\n",
    "    model.to('cuda')\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old, not gene-aware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        for in_size, out_size in zip([input_dim]+hidden_dims, hidden_dims):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.LayerNorm(out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "            modules.append(nn.Dropout(p=0.5))\n",
    "        modules.append(nn.Linear(hidden_dims[-1], out_dim))\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with filtered dimensions\n",
    "use_cuda = torch.cuda.is_available()\n",
    "hidden_dims = [2048, 1024, 512, 256, 128]\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=genes_X.shape[1],  # filtered gene count\n",
    "    hidden_dims=hidden_dims,\n",
    "    out_dim=isoforms_Y_proportions.shape[1]  # filtered isoform count\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "if use_cuda:\n",
    "    model.to('cuda')\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonCorrelationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Pearson correlation loss: returns 1 - correlation\n",
    "    Minimizing this maximizes correlation between predictions and targets\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Flatten to compute global correlation\n",
    "        pred_flat = pred.flatten()\n",
    "        target_flat = target.flatten()\n",
    "        \n",
    "        # Center the variables\n",
    "        pred_centered = pred_flat - pred_flat.mean()\n",
    "        target_centered = target_flat - target_flat.mean()\n",
    "        \n",
    "        # Compute correlation\n",
    "        numerator = (pred_centered * target_centered).sum()\n",
    "        denominator = torch.sqrt((pred_centered ** 2).sum() * (target_centered ** 2).sum()) + 1e-8\n",
    "        \n",
    "        correlation = numerator / denominator\n",
    "        \n",
    "        # Return 1 - correlation (to minimize)\n",
    "        return 1 - correlation\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines MSE and Pearson correlation loss\n",
    "    \"\"\"\n",
    "    def __init__(self, mse_weight=0.5, corr_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.corr_weight = corr_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.corr_loss = PearsonCorrelationLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        corr = self.corr_loss(pred, target)\n",
    "        return self.mse_weight * mse + self.corr_weight * corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, mode=\"min\"):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            patience (int): number of epochs to wait after last improvement\n",
    "            min_delta (float): minimum change to qualify as improvement\n",
    "            mode (str): \"min\" to monitor decreasing metric, \"max\" for increasing\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            return False\n",
    "\n",
    "        improvement = (\n",
    "            (current_score < self.best_score - self.min_delta)\n",
    "            if self.mode == \"min\"\n",
    "            else (current_score > self.best_score + self.min_delta)\n",
    "        )\n",
    "\n",
    "        if improvement:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    \"\"\"Compute Pearson correlation coefficient\"\"\"\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "    return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + 1e-8)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, num_epochs, scheduler, early_stopping, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses_array = []\n",
    "    val_losses_array = []\n",
    "    train_corr_array = []\n",
    "    val_corr_array = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ----- TRAINING -----\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corr = 0.0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            with torch.no_grad():\n",
    "                batch_corr = pearson_corr(outputs.flatten(), targets.flatten()).item()\n",
    "                running_corr += batch_corr * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_corr = running_corr / total\n",
    "        train_losses_array.append(train_loss)\n",
    "        train_corr_array.append(train_corr)\n",
    "\n",
    "        # ----- VALIDATION -----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corr = 0.0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                batch_corr = pearson_corr(outputs.flatten(), targets.flatten()).item()\n",
    "                val_corr += batch_corr * inputs.size(0)\n",
    "                total_val += inputs.size(0)\n",
    "\n",
    "        val_loss /= total_val\n",
    "        val_corr /= total_val\n",
    "        val_losses_array.append(val_loss)\n",
    "        val_corr_array.append(val_corr)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Corr: {train_corr:.4f} \"\n",
    "              f\"| Val Loss: {val_loss:.4f} | Val Corr: {val_corr:.4f}\")\n",
    "\n",
    "    print(\"Finished training.\")\n",
    "    return train_losses_array, train_corr_array, val_losses_array, val_corr_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 500\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Use combined loss (MSE + Correlation)\n",
    "loss_fn = CombinedLoss(mse_weight=0.3, corr_weight=0.7)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=1e-4, mode=\"min\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "t0 = time.time()\n",
    "\n",
    "train_losses, train_corrs, val_losses, val_corrs = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    num_epochs,\n",
    "    scheduler,\n",
    "    early_stopping,\n",
    "    device\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'\\nTotal training time: {(t1-t0)//60:.0f}m {(t1-t0)%60:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(val_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', color='tab:blue', linewidth=2)\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', color='tab:orange', linewidth=2)\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Combined Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Correlation plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_corrs, label='Train Correlation', color='tab:green', linewidth=2)\n",
    "plt.plot(epochs, val_corrs, label='Validation Correlation', color='tab:red', linewidth=2)\n",
    "plt.title('Correlation over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify that predictions sum to 1 for each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_gene_proportions(predictions, gene_to_isoform_map, sorted_gene_ids, n_samples=5):\n",
    "    predictions = predictions.squeeze(1)  # [batch, n_isoforms]\n",
    "    \n",
    "    violations = []\n",
    "    all_sums = []\n",
    "    \n",
    "    for gene_id in sorted_gene_ids[:10]:\n",
    "        isoform_indices = gene_to_isoform_map[gene_id]\n",
    "        \n",
    "        # Get proportions for this gene across all samples\n",
    "        gene_proportions = predictions[:n_samples, isoform_indices]  # [n_samples, n_isoforms_for_gene]\n",
    "        \n",
    "        sums = gene_proportions.sum(dim=1)\n",
    "        all_sums.extend(sums.tolist())\n",
    "        mean_sum = sums.mean().item()\n",
    "        max_deviation = (sums - 1.0).abs().max().item()\n",
    "        \n",
    "        status = \"ok\" if max_deviation < 1e-5 else \"not ok\"\n",
    "        \n",
    "        print(f\"{status} Gene {gene_id}: mean_sum={mean_sum:.6f}, max_deviation={max_deviation:.2e}, n_isoforms={len(isoform_indices)}\")\n",
    "        \n",
    "        if max_deviation >= 1e-5:\n",
    "            violations.append((gene_id, max_deviation))\n",
    "    \n",
    "    if violations:\n",
    "        print(f\"\\n  Found {len(violations)} genes with sum != 1.0\")\n",
    "    else:\n",
    "        print(f\"\\n All checked genes have proportions summing to 1.0!\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    all_sums_tensor = torch.tensor(all_sums)\n",
    "    print(f\"\\nOverall statistics:\")\n",
    "    print(f\"  Mean sum: {all_sums_tensor.mean():.6f}\")\n",
    "    print(f\"  Std sum: {all_sums_tensor.std():.6f}\")\n",
    "    print(f\"  Min sum: {all_sums_tensor.min():.6f}\")\n",
    "    print(f\"  Max sum: {all_sums_tensor.max():.6f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_inputs, sample_targets = next(iter(validation_loader))\n",
    "    sample_inputs = sample_inputs.float().to(device)\n",
    "    sample_predictions = model(sample_inputs).cpu()\n",
    "\n",
    "verify_gene_proportions(sample_predictions, gene_to_isoform_map, sorted_gene_ids, n_samples=min(5, len(sample_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_corrs': train_corrs,\n",
    "    'val_corrs': val_corrs,\n",
    "    'n_genes': genes_X.shape[1],\n",
    "    'n_isoforms': isoforms_Y_proportions.shape[1],\n",
    "    'gene_to_isoform_map': gene_to_isoform_map,\n",
    "    'sorted_gene_ids': sorted_gene_ids,\n",
    "    'hidden_dims': hidden_dims,\n",
    "}, 'isoform_model_gene_aware.pt')\n",
    "\n",
    "print(\"Model saved to 'isoform_model_gene_aware.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds_list, targets_list = [], []\n",
    "n_points = 5000\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds_list.append(outputs.cpu())\n",
    "        targets_list.append(targets.cpu())\n",
    "\n",
    "preds = torch.cat(preds_list).flatten()\n",
    "targets = torch.cat(targets_list).flatten()\n",
    "\n",
    "if len(preds) > n_points:\n",
    "    idx = torch.randperm(len(preds))[:n_points]\n",
    "    preds = preds[idx]\n",
    "    targets = targets[idx]\n",
    "\n",
    "residuals = targets - preds\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(residuals, alpha=0.3, bins=100)\n",
    "plt.axvline(0, color=\"darkred\", linestyle=\"--\")\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(preds, residuals, alpha=0.3, s=10)\n",
    "plt.axhline(0, color=\"darkred\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted Isoform Proportion\")\n",
    "plt.ylabel(\"Residual (True - Predicted)\")\n",
    "plt.title(\"Residual vs Predicted\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(preds, targets, alpha=0.2, s=10, c='darkred')\n",
    "plt.plot([preds.min(), preds.max()], [preds.min(), preds.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Predicted Isoform Proportion\")\n",
    "plt.ylabel(\"True Isoform Proportion\")\n",
    "plt.title(\"Predicted vs True\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "rmse = torch.sqrt(torch.mean((preds - targets) ** 2))\n",
    "corr = pearson_corr(preds, targets)\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Pearson Correlation: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_corrs': train_corrs,\n",
    "    'val_corrs': val_corrs,\n",
    "    'n_genes': genes_X.shape[1],\n",
    "    'n_isoforms': isoforms_Y_proportions.shape[1],\n",
    "}, 'isoform_model_improved.pt')\n",
    "\n",
    "print(\"Model saved to 'isoform_model_improved.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
